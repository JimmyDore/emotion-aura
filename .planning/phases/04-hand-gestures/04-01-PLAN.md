---
phase: 04-hand-gestures
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ml/HandDetector.ts
  - src/main.ts
autonomous: true

must_haves:
  truths:
    - "HandLandmarker initializes from the pre-downloaded hand model buffer without errors"
    - "Hand detection runs on alternating video frames (odd frames) while face detection runs on even frames"
    - "Face detection still produces valid emotion results on its even-frame cadence"
  artifacts:
    - path: "src/ml/HandDetector.ts"
      provides: "HandLandmarker wrapper with init/detect/close lifecycle"
      exports: ["HandDetector"]
    - path: "src/main.ts"
      provides: "Staggered face/hand inference in render loop"
      contains: "handDetector"
  key_links:
    - from: "src/main.ts"
      to: "src/ml/HandDetector.ts"
      via: "handDetector.init() and handDetector.detect()"
      pattern: "handDetector\\.detect"
    - from: "src/main.ts"
      to: "src/ml/ModelLoader.ts"
      via: "modelLoader.getHandModelBuffer()"
      pattern: "getHandModelBuffer"
---

<objective>
Create HandDetector wrapper for MediaPipe HandLandmarker and wire staggered face/hand inference into the render loop.

Purpose: Enable hand landmark detection alongside face detection without performance degradation by alternating which ML model runs on each new video frame.
Output: HandDetector class + main.ts with frame-alternating inference (face on even frames, hand on odd frames).
</objective>

<execution_context>
@/Users/jimmydore/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jimmydore/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-hand-gestures/04-RESEARCH.md

@src/ml/FaceDetector.ts
@src/ml/ModelLoader.ts
@src/main.ts
@src/core/constants.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create HandDetector class</name>
  <files>src/ml/HandDetector.ts</files>
  <action>
Create `src/ml/HandDetector.ts` mirroring the FaceDetector pattern exactly:

```typescript
import { HandLandmarker, FilesetResolver } from '@mediapipe/tasks-vision';
import type { HandLandmarkerResult } from '@mediapipe/tasks-vision';
```

Class `HandDetector` with:
- Private `landmarker: HandLandmarker | null = null`
- Private `lastVideoTime = -1`
- `async init(modelBuffer: Uint8Array, wasmCdnPath: string): Promise<void>` -- calls `FilesetResolver.forVisionTasks(wasmCdnPath)`, then `HandLandmarker.createFromOptions(vision, {...})` with:
  - `baseOptions.modelAssetBuffer: modelBuffer`
  - `baseOptions.delegate: 'GPU'`
  - `runningMode: 'VIDEO'`
  - `numHands: 1` (user decision: one hand only)
  - `minHandDetectionConfidence: 0.5`
  - `minTrackingConfidence: 0.5`
- `detect(video: HTMLVideoElement): HandLandmarkerResult | null` -- returns null if landmarker not initialized OR `video.currentTime === this.lastVideoTime` (stale frame). Otherwise calls `this.landmarker.detectForVideo(video, performance.now())`.
- `close(): void` -- calls `this.landmarker?.close()`, sets to null.

Note on FilesetResolver: Accept a second `forVisionTasks()` call. The WASM CDN URL is identical to FaceDetector's -- browser HTTP cache serves it instantly on the second load. Do NOT try to share the WasmFileset instance between detectors (adds coupling for zero benefit).
  </action>
  <verify>Run `npx tsc --noEmit` from project root -- no type errors.</verify>
  <done>HandDetector.ts exports a class with init/detect/close matching FaceDetector's pattern, configured for single-hand VIDEO mode detection.</done>
</task>

<task type="auto">
  <name>Task 2: Wire staggered inference into main.ts render loop</name>
  <files>src/main.ts</files>
  <action>
Modify `src/main.ts` to add hand detection with frame-alternating inference:

**1. Add imports at the top:**
```typescript
import { HandDetector } from './ml/HandDetector.ts';
```

**2. Add module-level variable:**
```typescript
let handDetector: HandDetector | null = null;
```

**3. In `loadAndConnect()`, after `faceDetector.init(...)` (line ~115), add HandDetector initialization:**
```typescript
handDetector = new HandDetector();
await handDetector.init(modelLoader.getHandModelBuffer()!, WASM_CDN);
```

**4. In the `animate()` function, replace the current face detection block:**

Current code (approximately lines 172-185):
```typescript
const result = faceDetector!.detect(video!);
if (result !== null) {
  if (result.faceBlendshapes && result.faceBlendshapes.length > 0) { ... }
  else { emotionState.decayToNeutral(); ... }
}
```

Replace with staggered inference using a frame toggle:

Add a variable `let inferenceToggle = false;` next to the other `let` declarations before `animate()` (near `lastTime`, `spawnAccumulator`, `lastFaceLandmarks`).

New detection block:
```typescript
// Staggered inference: face on even new-video-frames, hand on odd
const faceResult = faceDetector!.detect(video!);
const handResult = handDetector!.detect(video!);

// Only one returns non-null per new video frame due to alternation.
// On stale render frames (60fps render vs 30fps video), both return null.
if (faceResult !== null || handResult !== null) {
  if (inferenceToggle) {
    // Hand frame -- but we called both detectors above.
    // Actually: stagger at the CALL SITE to avoid wasting inference.
  }
  inferenceToggle = !inferenceToggle;
}
```

Wait -- the research says stagger at the call site, calling ONLY one detector per new video frame. But both detectors have internal stale-frame checks (`video.currentTime === lastVideoTime`). If we call both, only one sees a "new" frame (the one that hasn't been called since this video frame).

Correct approach: Track new video frames explicitly and call only one detector:

```typescript
// Track video frame changes for staggered inference
let lastVideoTime = -1;
let inferenceToggle = false;
```

Then in `animate()`:
```typescript
// Detect new video frame (30fps video in 60fps render loop)
const isNewVideoFrame = video!.currentTime !== lastVideoTime;

if (isNewVideoFrame) {
  lastVideoTime = video!.currentTime;

  if (!inferenceToggle) {
    // FACE frame (even)
    const result = faceDetector!.detect(video!);
    if (result !== null) {
      if (result.faceBlendshapes && result.faceBlendshapes.length > 0) {
        const rawScores = emotionClassifier.classify(result.faceBlendshapes[0].categories);
        emotionState.update(rawScores);
        lastFaceLandmarks = result.faceLandmarks?.[0];
      } else {
        emotionState.decayToNeutral();
        lastFaceLandmarks = undefined;
      }
    }
  } else {
    // HAND frame (odd) -- detection only, processing deferred to Plan 04-03
    const handResultData = handDetector!.detect(video!);
    // TODO(04-03): Process hand landmarks for gesture classification
    void handResultData; // Suppress unused warning until Plan 04-03 wires gesture pipeline
  }

  inferenceToggle = !inferenceToggle;
}
```

Remove the old `const result = faceDetector!.detect(video!)` block and its associated `if (result !== null)` block. The new code replaces it entirely.

IMPORTANT: The `lastVideoTime` variable used for stagger tracking is separate from the one inside each detector. Each detector still has its own internal `lastVideoTime` for its own stale-frame check. The outer `lastVideoTime` determines WHICH detector gets called.

**5. In HMR cleanup, add handDetector disposal:**
```typescript
if (handDetector) {
  handDetector.close();
  handDetector = null;
}
```

Add this after the existing `faceDetector` cleanup block.
  </action>
  <verify>
1. `npx tsc --noEmit` passes with no errors.
2. `npx vite build` completes successfully.
3. Visual check: Run `npx vite` and open in browser -- face detection still works (emotion overlay shows emotions), hand model initializes without console errors. Hand detection results are logged but not yet processed (deferred to Plan 04-03).
  </verify>
  <done>Main.ts alternates face/hand inference on new video frames. Face detection pipeline (emotion classification, landmark tracking, particle spawning) still functions correctly on its half-rate cadence. HandDetector initialized and detecting, results ready for gesture pipeline in Plan 04-03.</done>
</task>

</tasks>

<verification>
1. TypeScript compiles without errors: `npx tsc --noEmit`
2. Vite builds successfully: `npx vite build`
3. Browser test: Emotion overlay still shows correct emotions (face runs on even frames)
4. Browser test: No console errors related to HandLandmarker initialization
5. Performance: No noticeable FPS drop (staggered inference keeps per-frame ML cost at one model)
</verification>

<success_criteria>
- HandDetector class created, mirrors FaceDetector pattern
- Staggered inference wired in main.ts (face even frames, hand odd frames)
- Emotion pipeline still works correctly at half-rate face detection
- No TypeScript or build errors
- Hand detection results available (but not yet processed -- deferred to Plan 04-03)
</success_criteria>

<output>
After completion, create `.planning/phases/04-hand-gestures/04-01-SUMMARY.md`
</output>
