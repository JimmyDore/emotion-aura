---
phase: 02-emotion-detection
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/state/EmotionState.ts
  - src/ui/EmotionOverlay.ts
  - src/main.ts
  - src/style.css
autonomous: false

must_haves:
  truths:
    - "User sees visible feedback that face tracking is active (indicator changes when face enters/leaves frame)"
    - "User sees their current emotion label update in real-time as they change expressions"
    - "Emotion transitions are smooth (no flickering between states, gradual morph over 1-2 seconds)"
    - "Wider smile produces a visibly stronger happy signal than a slight smile"
    - "When user moves out of frame, display gracefully falls back to neutral"
  artifacts:
    - path: "src/state/EmotionState.ts"
      provides: "EMA smoothing for emotion scores, face-lost decay, current state accessor"
      exports: ["EmotionState"]
    - path: "src/ui/EmotionOverlay.ts"
      provides: "DOM overlay showing emotion label, intensity bar, and face detection indicator"
      exports: ["EmotionOverlay"]
    - path: "src/main.ts"
      provides: "Full detection pipeline wired into render loop"
      contains: "faceDetector"
  key_links:
    - from: "src/main.ts"
      to: "src/ml/FaceDetector.ts"
      via: "faceDetector.init() after models load, faceDetector.detect() in render loop"
      pattern: "faceDetector\\.detect"
    - from: "src/main.ts"
      to: "src/ml/EmotionClassifier.ts"
      via: "classifier.classify() on blendshape data each frame"
      pattern: "classifier\\.classify"
    - from: "src/main.ts"
      to: "src/state/EmotionState.ts"
      via: "emotionState.update() with raw scores, emotionState.decayToNeutral() when no face"
      pattern: "emotionState\\.(update|decayToNeutral)"
    - from: "src/main.ts"
      to: "src/ui/EmotionOverlay.ts"
      via: "overlay.update() with smoothed EmotionResult each frame"
      pattern: "overlay\\.update"
    - from: "src/state/EmotionState.ts"
      to: "src/core/constants.ts"
      via: "imports EMA_ALPHA and EMA_ALPHA_FACE_LOST"
      pattern: "EMA_ALPHA"
---

<objective>
Wire the complete emotion detection pipeline: EMA-smoothed emotion state, visual overlay showing the detected emotion, and main.ts integration connecting FaceDetector -> EmotionClassifier -> EmotionState -> EmotionOverlay in the render loop.

Purpose: This plan takes the detection modules from Plan 01 and makes them visible to the user. After this plan, the user sees their face tracked with a detection indicator, their emotion classified and displayed with smooth transitions, and graceful neutral fallback when they leave frame. This completes all 5 Phase 2 success criteria.

Output: 2 new modules (EmotionState.ts, EmotionOverlay.ts), modified main.ts with full pipeline, updated styles, and human-verified emotion detection.
</objective>

<execution_context>
@/Users/jimmydore/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jimmydore/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-emotion-detection/02-RESEARCH.md
@.planning/phases/02-emotion-detection/02-01-SUMMARY.md

@src/core/types.ts
@src/core/constants.ts
@src/ml/FaceDetector.ts
@src/ml/EmotionClassifier.ts
@src/ml/ModelLoader.ts
@src/main.ts
@src/style.css
</context>

<tasks>

<task type="auto">
  <name>Task 1: EmotionState smoother and EmotionOverlay UI</name>
  <files>src/state/EmotionState.ts, src/ui/EmotionOverlay.ts, src/style.css</files>
  <action>
**Create src/state/EmotionState.ts** -- EMA smoother + state manager:

1. **Constructor**: Initialize smoothed scores to `{ happy: 0, sad: 0, angry: 0, surprised: 0, neutral: 1 }` and `faceDetected = false`.

2. **update(raw: EmotionScores)**: Apply EMA smoothing using `EMA_ALPHA` from constants.ts. Formula: `smoothed[key] += alpha * (raw[key] - smoothed[key])` for each emotion key. Set `faceDetected = true`. Return the current EmotionResult (call extractDominant internally).

3. **decayToNeutral()**: Same as update but with the neutral target `{ happy: 0, sad: 0, angry: 0, surprised: 0, neutral: 1 }` and using `EMA_ALPHA_FACE_LOST` (faster decay per research recommendation). Set `faceDetected = false`. Return EmotionResult.

4. **getCurrent()**: Returns the current `EmotionResult` without modifying state. Useful for reading between inference frames (render loop runs at 60fps, inference at 30fps).

5. **extractDominant()** (private): Find the emotion with highest smoothed score. Return `{ dominant, intensity: Math.min(1, maxScore) }`.

6. **getResult()** (private helper): Assembles and returns `EmotionResult` from current smoothed scores, dominant, intensity, and faceDetected.

Import `EMA_ALPHA`, `EMA_ALPHA_FACE_LOST` from constants. Import `EmotionScores`, `EmotionResult`, `EmotionType` from types.

**Create src/ui/EmotionOverlay.ts** -- DOM overlay for emotion display:

Create a minimal, clean overlay positioned in the top-right area of the viewport. The overlay should feel like a subtle HUD, not a debug panel. It contains:

1. **Face detection indicator**: A small colored dot -- green when face is detected, fading to gray when lost. Use CSS transition for the color change (smooth, not instant).

2. **Emotion label**: Text showing the current dominant emotion (e.g., "happy", "surprised"). Use CSS text-transform capitalize for clean display. The label should transition opacity slightly when the dominant emotion changes.

3. **Intensity bar**: A thin horizontal bar (about 80-100px wide) that fills proportionally to the intensity value (0-1). Color the bar based on the current emotion:
   - happy: warm gold/amber (#F59E0B)
   - sad: cool blue (#3B82F6)
   - angry: red/orange (#EF4444)
   - surprised: bright purple (#A855F7)
   - neutral: soft gray (#9CA3AF)

4. **Constructor(container: HTMLElement)**: Creates the overlay DOM elements and appends to container. Use standard DOM creation (createElement), not innerHTML. Add a CSS class for styling.

5. **update(result: EmotionResult)**: Updates the indicator dot color, emotion label text, and intensity bar width/color. This is called every frame, so keep DOM writes minimal -- only update if values actually changed (compare with previous state to avoid unnecessary reflows).

6. **dispose()**: Removes the overlay element from the DOM. For HMR cleanup.

**In src/style.css**, add styles for the emotion overlay:
- Position: fixed, top-right corner with some padding (e.g., top: 60px to clear stats.js, right: 16px)
- Background: semi-transparent dark (rgba(0,0,0,0.6)) with backdrop-blur for glass effect
- Border-radius: 12px, padding: 12px 16px
- Font: system font stack, white text, small size (13-14px)
- z-index: 50 (below stats.js at 100)
- The detection dot: 8px circle, with CSS transition on background-color (0.5s ease)
- The intensity bar: height 4px, border-radius 2px, background transition on width and color (0.3s ease)
- Use `.emotion-overlay` as the root class

Keep styles minimal and clean -- this is a portfolio piece.
  </action>
  <verify>Run `npx tsc --noEmit`. Both new files compile. Verify EmotionState exports update(), decayToNeutral(), getCurrent(). Verify EmotionOverlay exports constructor(container), update(result), dispose(). Run `npx vite build` to confirm no bundling issues.</verify>
  <done>EmotionState smooths raw scores via EMA with face-lost decay. EmotionOverlay renders a clean HUD with detection indicator, emotion label, and intensity bar. Both compile and build cleanly.</done>
</task>

<task type="auto">
  <name>Task 2: Wire detection pipeline into main.ts render loop</name>
  <files>src/main.ts</files>
  <action>
Modify `src/main.ts` to integrate the full emotion detection pipeline. Changes:

1. **New imports** (add at top):
   ```
   import { FaceDetector } from './ml/FaceDetector.ts';
   import { EmotionClassifier } from './ml/EmotionClassifier.ts';
   import { EmotionState } from './state/EmotionState.ts';
   import { EmotionOverlay } from './ui/EmotionOverlay.ts';
   import { WASM_CDN } from './core/constants.ts';
   ```

2. **New module-level variables** (alongside existing sceneManager, cameraManager, etc.):
   ```
   let faceDetector: FaceDetector | null = null;
   let emotionOverlay: EmotionOverlay | null = null;
   ```
   Note: EmotionClassifier and EmotionState are instantiated locally since they don't need HMR cleanup.

3. **Initialize FaceDetector after models load** -- in the `loadAndConnect` function, AFTER `loadingScreen.hide()` and BEFORE the live experience section, add:
   ```
   // Initialize face detection from pre-downloaded model
   faceDetector = new FaceDetector();
   await faceDetector.init(modelLoader.getFaceModelBuffer()!, WASM_CDN);
   ```
   Place this after `loadingScreen.hide()` but before the video/canvas setup. The `!` non-null assertion is safe because loadAll() succeeded (we're past the try/catch).

4. **Create EmotionClassifier, EmotionState, EmotionOverlay** in the live experience section (after canvas/sceneManager setup, before the render loop):
   ```
   const emotionClassifier = new EmotionClassifier();
   const emotionState = new EmotionState();
   emotionOverlay = new EmotionOverlay(app);
   ```

5. **Modify the render loop** to include the detection pipeline. Replace the existing `animate()` function with:
   ```typescript
   function animate(): void {
     stats.begin();

     // Run face detection (skips if video frame unchanged)
     const result = faceDetector!.detect(video);

     if (result && result.faceBlendshapes && result.faceBlendshapes.length > 0) {
       const rawScores = emotionClassifier.classify(result.faceBlendshapes[0].categories);
       emotionState.update(rawScores);
     } else {
       emotionState.decayToNeutral();
     }

     // Update overlay every frame (reads smoothed state)
     emotionOverlay!.update(emotionState.getCurrent());

     sceneManager!.render();
     stats.end();
     rafId = requestAnimationFrame(animate);
   }
   ```

6. **HMR cleanup** -- in the existing `import.meta.hot.dispose` callback, add:
   ```
   if (faceDetector) {
     faceDetector.close();
     faceDetector = null;
   }
   if (emotionOverlay) {
     emotionOverlay.dispose();
     emotionOverlay = null;
   }
   ```

IMPORTANT: Do NOT remove any existing code (sceneManager, cameraManager, stats, error handling). Only ADD the detection pipeline alongside the existing render loop logic.

IMPORTANT: The detection runs on every animation frame, but FaceDetector.detect() internally skips when video.currentTime has not changed. This means inference runs at video framerate (~30fps) while the render loop runs at display framerate (~60fps). On non-inference frames, the overlay still updates from the smoothed state (getCurrent), keeping the UI smooth.
  </action>
  <verify>
1. Run `npx tsc --noEmit` -- zero errors.
2. Run `npx vite build` -- successful build with no warnings.
3. Run `npx vite` (dev server) and open in browser. Grant camera permission.
4. Verify: Face detection indicator appears (green dot when face visible, gray when face is out of frame).
5. Verify: Emotion label updates when making deliberate expressions (smile = happy, frown = sad, wide eyes + open mouth = surprised).
6. Verify: No console errors related to MediaPipe, blendshapes, or type mismatches.
  </verify>
  <done>main.ts initializes FaceDetector from pre-downloaded model buffer, creates classifier/state/overlay, and runs the full detection pipeline in the render loop. Face detection indicator, emotion label, and intensity bar are visible. HMR cleanup handles FaceDetector.close() and overlay disposal.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete emotion detection pipeline: face tracking with visual indicator, emotion classification (happy/sad/angry/surprised/neutral) with intensity scaling, EMA-smoothed transitions, and graceful neutral fallback when face leaves frame.</what-built>
  <how-to-verify>
1. Open the app in browser (run `npx vite` if not already running, visit http://localhost:5173)
2. Grant camera permission, wait for models to load
3. **Face detection indicator**: Look at the top-right overlay. You should see a green dot when your face is in frame. Move your face completely out of frame -- the dot should fade to gray.
4. **Emotion classification**: Make deliberate expressions:
   - SMILE broadly --> label should show "happy", intensity bar should fill with gold/amber
   - FROWN with inner brows raised --> label should show "sad", blue bar
   - FURROW brows down, sneer --> label should show "angry", red bar
   - OPEN mouth wide, raise eyebrows, widen eyes --> label should show "surprised", purple bar
   - RELAX face --> label should return to "neutral", gray bar
5. **Smooth transitions**: Rapidly switch between expressions. The label and bar should transition smoothly over ~1-2 seconds, no flickering or instant jumps.
6. **Intensity scaling**: Compare a slight smile vs a wide grin. The intensity bar should be noticeably shorter for the slight smile.
7. **Face lost fallback**: Cover your face or move out of frame. The system should gradually decay to "neutral" (not freeze or glitch).
8. **Performance**: Check stats.js in the top-left corner. FPS should remain above 30 with face detection active.
  </how-to-verify>
  <resume-signal>Type "approved" or describe any issues with detection accuracy, transition smoothness, or visual presentation.</resume-signal>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. `npx vite build` succeeds
3. All new files exist: src/state/EmotionState.ts, src/ui/EmotionOverlay.ts
4. main.ts has the complete pipeline: FaceDetector.init -> detect -> classify -> smooth -> overlay
5. Emotion overlay is visible with detection indicator, label, and intensity bar
6. All 5 emotions are classifiable with deliberate expressions
7. Transitions are smooth (EMA smoothing active, no flickering)
8. Face-lost gracefully decays to neutral
9. FPS remains above 30 with detection active
</verification>

<success_criteria>
- User sees a green indicator when their face is tracked, gray when lost
- User sees correct emotion labels for deliberate expressions (5 categories)
- Transitions morph smoothly over 1-2 seconds with no jarring snaps
- Wider expressions produce stronger intensity than subtle ones
- Moving out of frame decays gracefully to neutral
- All of the above confirmed by human verification
</success_criteria>

<output>
After completion, create `.planning/phases/02-emotion-detection/02-02-SUMMARY.md`
</output>
