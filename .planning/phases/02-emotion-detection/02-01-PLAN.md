---
phase: 02-emotion-detection
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/core/types.ts
  - src/core/constants.ts
  - src/ml/FaceDetector.ts
  - src/ml/EmotionClassifier.ts
autonomous: true

must_haves:
  truths:
    - "FaceLandmarker initializes from the pre-downloaded model buffer without errors"
    - "detectForVideo returns blendshape data when a face is in frame"
    - "Emotion classifier produces scores for all 5 emotions from blendshape data"
    - "Intensity scales with expression strength (wider smile = higher happy score)"
  artifacts:
    - path: "src/core/types.ts"
      provides: "EmotionType, EmotionScores, EmotionResult, FaceState types"
      contains: "EmotionType"
    - path: "src/core/constants.ts"
      provides: "Emotion blendshape weights, EMA alpha, classification thresholds"
      contains: "EMOTION_WEIGHTS"
    - path: "src/ml/FaceDetector.ts"
      provides: "FaceLandmarker wrapper with init() and detect() methods"
      exports: ["FaceDetector"]
    - path: "src/ml/EmotionClassifier.ts"
      provides: "Blendshape-to-emotion classification with weighted sums"
      exports: ["EmotionClassifier"]
  key_links:
    - from: "src/ml/FaceDetector.ts"
      to: "@mediapipe/tasks-vision"
      via: "FaceLandmarker.createFromOptions with modelAssetBuffer"
      pattern: "createFromOptions"
    - from: "src/ml/EmotionClassifier.ts"
      to: "src/core/constants.ts"
      via: "imports EMOTION_WEIGHTS for weighted sum calculation"
      pattern: "EMOTION_WEIGHTS"
    - from: "src/ml/FaceDetector.ts"
      to: "src/core/constants.ts"
      via: "imports WASM_CDN for FilesetResolver"
      pattern: "WASM_CDN"
---

<objective>
Add emotion detection types, FaceLandmarker wrapper, and rule-based emotion classifier -- the core ML pipeline for Phase 2.

Purpose: These modules form the detection backbone. FaceDetector wraps MediaPipe's FaceLandmarker for per-frame face inference with blendshape output. EmotionClassifier converts the 52 blendshape scores into 5 emotion categories with intensity. Together they produce the raw emotion data that Plan 02 will smooth and display.

Output: 2 new modules (FaceDetector.ts, EmotionClassifier.ts) + extended types and constants files.
</objective>

<execution_context>
@/Users/jimmydore/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jimmydore/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-emotion-detection/02-RESEARCH.md

@src/core/types.ts
@src/core/constants.ts
@src/ml/ModelLoader.ts
@src/main.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Emotion types and classifier constants</name>
  <files>src/core/types.ts, src/core/constants.ts</files>
  <action>
**In src/core/types.ts**, add these types AFTER the existing types (do not remove any existing types):

```typescript
/** The 5 emotion categories detected from facial expressions. */
export type EmotionType = 'happy' | 'sad' | 'angry' | 'surprised' | 'neutral';

/** Raw or smoothed scores for each emotion (0-1 range). */
export type EmotionScores = Record<EmotionType, number>;

/** Complete emotion result for a single frame. */
export interface EmotionResult {
  dominant: EmotionType;
  intensity: number;        // 0-1, strength of dominant emotion
  scores: EmotionScores;    // All 5 scores (for blending in Phase 3)
  faceDetected: boolean;
}

/** Blendshape weight definition for emotion classification. */
export interface BlendshapeWeight {
  name: string;
  weight: number;
}
```

**In src/core/constants.ts**, add these constants AFTER the existing constants:

```typescript
import type { BlendshapeWeight } from './types.ts';

/** EMA smoothing factor. 0.2 gives ~1-1.5 second transition at 30fps. */
export const EMA_ALPHA = 0.2;

/** Faster decay alpha when face is lost -- respond quicker to absence. */
export const EMA_ALPHA_FACE_LOST = 0.3;

/**
 * Emotion-to-blendshape weighted mappings.
 * Based on FACS (Facial Action Coding System) action unit correlations.
 * Weights within each emotion sum to 1.0.
 * All values are tunable -- adjust here, not in classifier logic.
 */
export const EMOTION_WEIGHTS: Record<string, BlendshapeWeight[]> = {
  happy: [
    { name: 'mouthSmileLeft', weight: 0.35 },
    { name: 'mouthSmileRight', weight: 0.35 },
    { name: 'cheekSquintLeft', weight: 0.15 },
    { name: 'cheekSquintRight', weight: 0.15 },
  ],
  sad: [
    { name: 'mouthFrownLeft', weight: 0.30 },
    { name: 'mouthFrownRight', weight: 0.30 },
    { name: 'browInnerUp', weight: 0.40 },
  ],
  angry: [
    { name: 'browDownLeft', weight: 0.25 },
    { name: 'browDownRight', weight: 0.25 },
    { name: 'mouthFrownLeft', weight: 0.10 },
    { name: 'mouthFrownRight', weight: 0.10 },
    { name: 'noseSneerLeft', weight: 0.15 },
    { name: 'noseSneerRight', weight: 0.15 },
  ],
  surprised: [
    { name: 'jawOpen', weight: 0.25 },
    { name: 'eyeWideLeft', weight: 0.175 },
    { name: 'eyeWideRight', weight: 0.175 },
    { name: 'browInnerUp', weight: 0.133 },
    { name: 'browOuterUpLeft', weight: 0.133 },
    { name: 'browOuterUpRight', weight: 0.134 },
  ],
};

/** Multiplier for neutral score derivation: neutral = max(0, 1 - maxEmotionScore * this). */
export const NEUTRAL_SUPPRESSION_FACTOR = 1.5;
```

Note: The import statement must use `import type` to satisfy verbatimModuleSyntax. Place the import at the top of constants.ts.
  </action>
  <verify>Run `npx tsc --noEmit` from project root. Zero type errors. Verify the new types are exported and constants are importable.</verify>
  <done>types.ts exports EmotionType, EmotionScores, EmotionResult, BlendshapeWeight. constants.ts exports EMA_ALPHA, EMA_ALPHA_FACE_LOST, EMOTION_WEIGHTS, NEUTRAL_SUPPRESSION_FACTOR. TypeScript compiles cleanly.</done>
</task>

<task type="auto">
  <name>Task 2: FaceDetector -- MediaPipe FaceLandmarker wrapper</name>
  <files>src/ml/FaceDetector.ts</files>
  <action>
Create `src/ml/FaceDetector.ts` -- a wrapper around MediaPipe's FaceLandmarker that:

1. **init(modelBuffer, wasmCdnPath)**: Creates FaceLandmarker via `FaceLandmarker.createFromOptions(vision, options)` with:
   - `baseOptions.modelAssetBuffer` set to the Uint8Array from ModelLoader
   - `baseOptions.delegate: 'GPU'`
   - `runningMode: 'VIDEO'`
   - `numFaces: 1`
   - `outputFaceBlendshapes: true`
   - `outputFacialTransformationMatrixes: false`
   - Use `FilesetResolver.forVisionTasks(wasmCdnPath)` to get the vision fileset

2. **detect(video: HTMLVideoElement)**: Returns `FaceLandmarkerResult | null`
   - Track `lastVideoTime` (initialized to -1). Skip detection if `video.currentTime === lastVideoTime` (stale frame -- see Research pitfall #2).
   - Call `this.landmarker.detectForVideo(video, performance.now())` when frame is new.
   - Return null if landmarker not initialized or frame is stale.

3. **close()**: Calls `landmarker.close()` and nulls the reference. For HMR cleanup.

Import `FaceLandmarker`, `FilesetResolver` from `'@mediapipe/tasks-vision'`.
Import `FaceLandmarkerResult` as a type import for the return type annotation.
Import `WASM_CDN` from `'../core/constants.ts'` -- but accept wasmCdnPath as parameter for testability.

IMPORTANT: Use `createFromOptions` (NOT `createFromModelBuffer`) because only createFromOptions allows setting `outputFaceBlendshapes: true`. This is the #1 pitfall from research.

IMPORTANT: Always look up blendshapes by `categoryName`, never by array index. Index 0 is `_neutral` (53 total items). This class doesn't directly access blendshapes, but document this for downstream consumers.
  </action>
  <verify>Run `npx tsc --noEmit`. FaceDetector.ts compiles with no errors. Verify the class exports `init`, `detect`, `close` methods. Verify `outputFaceBlendshapes: true` is set in createFromOptions call.</verify>
  <done>FaceDetector class wraps FaceLandmarker with init/detect/close. Uses createFromOptions with blendshapes enabled. Tracks lastVideoTime to skip stale frames. TypeScript compiles cleanly.</done>
</task>

<task type="auto">
  <name>Task 3: EmotionClassifier -- blendshape scores to emotion categories</name>
  <files>src/ml/EmotionClassifier.ts</files>
  <action>
Create `src/ml/EmotionClassifier.ts` -- a stateless classifier that converts MediaPipe blendshape categories into emotion scores:

1. **classify(categories: Category[])**: Returns `EmotionScores`
   - Build a `Map<string, number>` from `categoryName` to `score` for O(1) lookup. ALWAYS use categoryName, never array index (Research pitfall #6: index 0 is `_neutral`).
   - For each emotion in EMOTION_WEIGHTS, compute weighted sum: `sum += get(blendshape.name) * blendshape.weight` for each BlendshapeWeight.
   - Compute neutral as: `Math.max(0, 1 - maxEmotionScore * NEUTRAL_SUPPRESSION_FACTOR)` where maxEmotionScore is `Math.max(happy, sad, angry, surprised)`.
   - Return the EmotionScores object with all 5 values.

2. **extractDominant(scores: EmotionScores)**: Returns `{ dominant: EmotionType; intensity: number }`
   - Iterate all entries in scores, find the one with highest value.
   - Return `{ dominant, intensity: Math.min(1, maxScore) }`.

Import `Category` type from `'@mediapipe/tasks-vision'` (type-only import).
Import `EMOTION_WEIGHTS`, `NEUTRAL_SUPPRESSION_FACTOR` from `'../core/constants.ts'`.
Import `EmotionScores`, `EmotionType` from `'../core/types.ts'`.

The classifier is intentionally stateless -- no smoothing here. Smoothing happens in EmotionState (Plan 02). This separation keeps classification pure and testable.
  </action>
  <verify>Run `npx tsc --noEmit`. EmotionClassifier.ts compiles with no errors. Verify classify() returns EmotionScores type, extractDominant() returns { dominant, intensity }. Verify EMOTION_WEIGHTS is imported from constants (not hardcoded).</verify>
  <done>EmotionClassifier converts blendshape categories to 5 emotion scores using weighted sums from constants.ts. extractDominant returns the strongest emotion with intensity. Stateless, pure, TypeScript-clean.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. All new files exist: src/ml/FaceDetector.ts, src/ml/EmotionClassifier.ts
3. All modified files compile: src/core/types.ts, src/core/constants.ts
4. `npx vite build` succeeds (no runtime import issues)
5. FaceDetector uses createFromOptions (not createFromModelBuffer)
6. EmotionClassifier reads weights from constants.ts (not hardcoded)
7. No blendshape access by array index (all by categoryName)
</verification>

<success_criteria>
- TypeScript compiles cleanly with all new code
- FaceDetector wraps FaceLandmarker with blendshapes enabled, stale frame detection, and GPU delegate
- EmotionClassifier produces weighted emotion scores from blendshape data using tunable constants
- All emotion weights and thresholds live in constants.ts (single source of truth)
- Vite build succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/02-emotion-detection/02-01-SUMMARY.md`
</output>
